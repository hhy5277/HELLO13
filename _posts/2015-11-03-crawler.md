---
layout: post
title: Web Crawling For Hearthstone World Championship
tag: [Python]
addr: Meituan, Beijing
description: Python Web Crawler For Hearthstone World Championship
keywords: python,有田十三
---

2015 Hearthstone World Championship is the Global Finals of 2015 HWC series tournaments. 16 players will compete, qualifying throughout 2015 at each regional championship. Matches are played at Anaheim, California at BlizzCon 2015 on November 6-7th.

<!--more-->

### What's the so?

[GOSUGAMERS](http://www.gosugamers.net/hearthstone/rankings) holds the Hearthstone rankings, when checked the score of __Thijs__, I found there have the high quality of the CARDs!!!

Well, let me call a crawler to help me download them.

## Prepare

Python looks better, install the module first.

> pip install request beautifulsoup4 urllib3

## Implementing

Few words, codes below:

{% highlight python linenos %}
import os
import requests
import bs4
import urllib

host = 'http://www.gosugamers.net'
root_url = 'http://www.gosugamers.net/hearthstone/cards'
root_path = '/Users/hello13/Documents/Proj/hearthstone/img'

# GET card names
def get_dir_names(url):
    response = requests.get(url)
    soup = bs4.BeautifulSoup(response.text, 'html.parser')
    return [a.attrs.get('href') for a in soup.select('div.card-list-cards-item a')]

# GET card download urls
def get_image_urls(url):
    response = requests.get(url)
    soup = bs4.BeautifulSoup(response.text, 'html.parser')
    return [img.attrs.get('src') for img in soup.select('div.card-item img[src^=/uploads]')]

for x in xrange(1,36):
    print('Page ' + str(x) + ' started...')
    index_url = root_url + '?page=' + str(x)

    for dir in get_dir_names(index_url):
        dir_name = dir.split('/')[1]
        dir_path = root_path + '/' + dir_name
        page_url = root_url + '/' + dir_name

        # if no exists, then make dirs
        if not os.path.exists(dir_path):
            os.mkdir(dir_path, 0755)

        for url in get_image_urls(page_url):
            card_url = host + url
            card_name = url.split('/')[4]
            card_path = dir_path + '/' + card_name
            # download images
            data = urllib.urlretrieve(card_url, card_path)

        print('Card: ' + dir_name + ' downloads completely.')

print('Cards all download completely.')
{% endhighlight%}

And then run `python crawler.py`
![img](/static/img/post/crawler-2.png)

"I dream and the world trembles", Ysera!!!
![img](/static/img/post/crawler-1.png)

Github repo: [link](https://github.com/yooungt13/yooungt13.github.com/blob/master/bin/crawler.py)

